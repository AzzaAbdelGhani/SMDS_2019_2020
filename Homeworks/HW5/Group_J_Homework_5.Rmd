---
title: "Homework 5"
author: "Group J - Abdalghani, Corti, Solomita"
date: "22/06/2020"
output:
  html_document:
    toc: yes
  beamer_presentation:
    highlight: tango
  include: null
  ioslides_presentation:
    highlight: tango
  pdf_document:
    highlight: tango
    keep_tex: yes
    toc: yes
  slide_level: 2
  slidy_presentation:
    fig.height: 3
    fig.width: 4
    highlight: tango
header-includes:
- \usepackage{color}
- \definecolor{Purple}{HTML}{911146}
- \definecolor{Orange}{HTML}{CF4A30}
- \setbeamercolor{alerted text}{fg=Orange}
- \setbeamercolor{frametitle}{bg=Purple}
institute: University of Udine & University of Trieste
graphics: yes
subtitle: 
fontsize: 10pt
---
```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.align = 'center', warning=FALSE, message=FALSE, fig.asp=0.625, dev='png', global.par = TRUE, dev.args=list(pointsize=10), fig.path = 'figs/')
library(MASS)
```
```{r setup, include=FALSE}
library(knitr)
local({
  hook_plot = knit_hooks$get('plot')
  knit_hooks$set(plot = function(x, options) {
    paste0('\n\n----\n\n', hook_plot(x, options))
  })
})
```

## CS: Exercise 1.3

**Suppose that $$ Y \sim \mathcal{N}(\begin{bmatrix} 1 \\2 \end{bmatrix} ,\begin{bmatrix} 2 & 1 \\ 1 & 2 \end{bmatrix}) $$ Find the conditional p.d.f. of $Y1$ given that $Y1 + Y2 = 3$.**

$Y$ is a $2 \times1$ random vector with distribution $\mathcal{N_2}(\mu,\Sigma)$ where $\mu = \begin{bmatrix} 1 \\2 \end{bmatrix}$  and $\Sigma = \begin{bmatrix} 2 & 1 \\ 1 & 2 \end{bmatrix}$. 

Reffering to (1.7) at section (1.6.2) in the book, $A Y \sim \mathcal{N}(A\mu, A\Sigma A^T)$, we can get that the joint distribution of 
$$AY = \begin{bmatrix} Y_1 \\ Y_1 + Y_2 \end{bmatrix}$$ 
is $\mathcal{N_2}(\mu^*, \Sigma^*)$, where $A$ is matrix of finite real constants : 
$$A = \begin{bmatrix} 1 & 0 \\ 1 & 1 \end{bmatrix}$$
The $2 \times 1$ mean vector $\mu^*$ is: 
$$\mu^* = A\mu =  \begin{bmatrix} 1 & 0 \\ 1 & 1 \end{bmatrix} \begin{bmatrix} 1 \\2 \end{bmatrix} = \begin{bmatrix} 1 \\3 \end{bmatrix}$$
and the $2 \times 2$ covariance matrix is : 
$$ \Sigma^*=A\Sigma A^T = \begin{bmatrix} 1 & 0 \\ 1 & 1 \end{bmatrix} \begin{bmatrix} 2 & 1 \\ 1 & 2 \end{bmatrix} \begin{bmatrix} 1 & 1 \\ 0 & 1 \end{bmatrix} = \begin{bmatrix} 2 & 3 \\ 3 & 6 \end{bmatrix} $$

In section (1.6.3), we got the conditional p.d.f of 2 random vectors with multivariate normal joint distribution. Thus we can apply this to the distribution of $AY$, the conditional distribution of $Y_1 | Y_1 + Y_2 = 3$ is $\mathcal{N}(\mu_c, \sigma^2_c)$ , where the conditional mean $\mu_c$ is : 
$$\mu_c = \mu^*_1+ \Sigma^*_{12} \Sigma^{*-1}_{22} (c_2 - \mu^*_2) = 1 + 3.(6)^{-1} .(3-3) = 1$$

and the conditional variance $\sigma^2_c$ is : 

$$\sigma^2_c = \Sigma^*_{11} - \Sigma^*_{12}\Sigma^{*-1}_{22}\Sigma^*_{21} = 2 - 3. (6)^{-1}.3 = 0.5$$

So, the conditional p.d.f. of $Y1$ given $Y1 + Y2 = 3$ is $\mathcal{N}(1,\frac{1}{2})$


## DAAG: Exercise 4.5

**The following code draws, in a 2×2 layout, 10 boxplots of random samples of 1000 from a normal distribution, 10 boxplots of random samples of 1000 from a t-distribution with 7 d.f., 10 boxplots of random samples of 200 from a normal distribution, and 10 boxplots of randomsamples of 200 from a t-distribution with 7 d.f. :**

```{r echo=TRUE,fig.show='hide',message=FALSE, warning=FALSE}
oldpar <- par(mfrow=c(2,2))
tenfold1000 <- rep(1:10, rep(1000,10))
boxplot(split(rnorm(1000*10), tenfold1000), ylab="normal - 1000")
boxplot(split(rt(1000*10, 7), tenfold1000),ylab=expression(t[7]*" - 1000"))
tenfold100 <- rep(1:10, rep(100, 10))
boxplot(split(rnorm(100*10), tenfold100), ylab="normal - 100")
boxplot(split(rt(100*10, 7), tenfold100),ylab=expression(t[7]*" - 100"))
par(oldpar)
```

**Refer back to the discussion of heavy-tailed distributions in Subsection 3.2.2, and comment on the different numbers and configurations of points that are flagged as possible outliers**.

```{r}
oldpar <- par(mfrow=c(2,2))
tenfold1000 <- rep(1:10, rep(1000,10))
boxplot(split(rnorm(1000*10), tenfold1000), ylab="normal - 1000")
boxplot(split(rt(1000*10, 7), tenfold1000),ylab=expression(t[7]*" - 1000"))
tenfold100 <- rep(1:10, rep(100, 10))
boxplot(split(rnorm(100*10), tenfold100), ylab="normal - 100")
boxplot(split(rt(100*10, 7), tenfold100),ylab=expression(t[7]*" - 100"))
par(oldpar)
```

Comparing normal and t-distribution we observe that there's a tendency to have a higher number of outliers in the samples simulated from a t-distribution rather than the normal.
This is due to the analytical form of the probability density function; in fact the pdf of a Normal reports an exponential decay and this property takes as consequence a probability that decays very rapidly as far as we go out of the mean $\mu$. This lead to have a very low probability to sample values that are outside the interval of the mean.
On the other hand, the t-distribution with 7 df does not have this property; these kind of distributions are known as **heavy-tailed distributions**. This lead to have an higher probability to sample points that are outside  the most frequent values.

In order to see how big is this difference consider this function that prints us the ratio $\frac{1-p_t(x \in [-a,+a])}{1-p_{norm}(x \in [-a,+a])}$ between the probabilities of a $t_7$ and a $\mathcal{N}(0,1)$ distribution to be out of a certain interval $[-a,+a]$ for $a =1,2,2.5,2.8,3$

```{r}
ratio_probability <- function(level){
  prob_norm <- 1-(pnorm(level,mean=0,sd=1)-pnorm(-level,mean=0,sd=1))
  prob_t <- 1-(pt(level,df=7)-pt(-level,df=7))
  return(prob_t/prob_norm)
}
```
```{r}
a<-c(1,2,2.5,2.8,3)

ratio_probability(a)
```

As we can observe from these probability ratios, a value out the region of $x\sim 0$ is more likely for the t-distribution and this difference is larger as far as we move out. In fact, the probability to sample a value $x \notin [-3,3]$ from a t-distribution is more than $7$ times larger than in the normal case.


## CS: Exercise 4.4

**Suppose that you have $n$ independent measurements of times between major aircraft disasters, $t_i$, and believe that the probability density function for the $t_i$’s is of the form: $f(t) = k e^{-\lambda t^2}$ $t \geq 0$ where $\lambda$ and $k$ are the same for all $i$.**

**(a) By considering the normal p.d.f., show that $k = \sqrt{4 \lambda / \pi}$. **

The normal p.d.f is : 

$$ f(t) = \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{1}{2}(\frac{(t - \mu)}{\sigma})^2} $$
By considering the mean of this normal p.d.f is $\mu = c \times t$ ,  where $c$ is a constant, then $f(t)$ will be : 

\begin{align}
f(t) &= \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{1}{2}(\frac{(t - ct)}{\sigma})^2}\\
&= \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{1}{2}(\frac{t(1 - c)}{\sigma})^2}\\
&= \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(1-c)^2}{2}(\frac{t^2}{\sigma^2})}\\
\end{align}


From this equation, we can consider $k = \frac{1}{\sqrt{2\pi\sigma^2}}$ and $\lambda = \frac{(1-c)^2}{2\sigma^2}$, then the f(t) will be as the p.d.f of $t_i$: 

$$f(t) = k e^{-\lambda t^2}$$ . 

Now, to prove that $k = \sqrt{4 \lambda / \pi}$, we can write $k$ with rescpect to $\lambda$ : 

$$k = \frac{1}{\sqrt{2\pi\sigma^2}} = \sqrt{\frac{1}{2\pi\sigma^2}} = \sqrt{\frac{1}{2\sigma^2}\frac{1}{\pi}} = \sqrt{\frac{\lambda}{(1-c)^2}\frac{1}{\pi}}$$

Finally, we can choose some possible values for $c$, in order to show that $k = \sqrt{4 \lambda / \pi}$. For example, if $c= \frac{1}{2}$ then : 


$$k =\sqrt{\frac{\lambda}{(1-\frac{1}{2})^2}\frac{1}{\pi}} = \sqrt{4 \lambda / \pi}$$

other possible value is $c = \frac{3}{2}$. 

**(b) Obtain a maximum likelihood estimator for $\lambda$. ** 

To obtain the MLE for $\lambda$ : 

1. Compute the likelihood function for the function f(t):

$$ L(t) = \prod_{i = 0}^nke^{-\lambda t_i^2} = k^n \prod_{i = 0}^n e^{-\lambda t_i^2}$$

2. Take the logarithm of the Likelihood function to get the log likelihood : 


$$ \ell(t) = \log (k^n \prod_{i = 0}^n e^{-\lambda t_i^2}) =  \log (k^n) + \log (\prod_{i = 0}^n e^{-\lambda t_i^2}) =n \log(k) - \lambda \sum_{i = 0}^n t_i^2 = n \log(\sqrt{4\lambda/\pi}) - \lambda \sum_{i = 0}^n t_i^2$$

3. Set the derivative of the log likelihood to 0:

$$\frac{d\ell(t)}{d\lambda} = \frac{n}{2\lambda} - \sum_{i = 0}^n t_i^2 = 0$$
$$\frac{n}{2\lambda} = \sum_{i = 0}^n t_i^2$$

$$\lambda = \frac{n}{2 \sum_{i = 0}^n t_i^2}$$
which is the Maximum Likelihood estimator for $\lambda$. 


**(c) Given observations of $T_i$ (in days) of: 243, 14, 121, 63, 45, 407 and 34, use a generalised likelihood ratio test to test $H_0 : \lambda  = 10^{-4}$ against the alternative of no restriction on $\lambda$ at the 5% significance level. Note that if $V ∼ χ_{1}^2$ then $Pr[V ≤ 3.841] = 0.95$. **




The log_likelihood is : $\ell(t) = n \log(\sqrt{4\lambda/\pi}) - \lambda \sum_{i = 0}^n t_i^2$, we need to replace $\lambda$ parameter once with the value of null hypothesis $\lambda  = 10^{-4}$ and the other with the value of MLE that we have found before as an alternative of no restriction on $\lambda$.
 
Then, the likelihood ratio test $LRT$ would become : 
$$ W(H_0) = 2 \{\ell(\hat\lambda) - \ell(\hat\lambda_{H_0})\} $$
 

```{r echo=TRUE,  message=FALSE, warning=FALSE}

log_likelihood <- function(data, param){
   length(data) * log(sqrt(4 * param / pi)) - param * sum(data^2)
 }

obs <- c(243, 14, 121, 63, 45, 407, 34)

lambda_mle <- length(obs) / (2 * sum(obs^2))
print(paste("MLE for lambda = ", lambda_mle))

l_H0 <- log_likelihood(obs, 10^-4)
l_mle <- log_likelihood(obs, lambda_mle)

lrt <- 2 * (l_mle - l_H0)

p_lrt <- pchisq(lrt, df=1, lower.tail = FALSE)
print(paste("p-value of Likelihood ratio test :", p_lrt))
```

Based on the p-value, as it is too low, we reject the null hypothesis. AS, also we can see that the MLE for $\lambda$ is different from $10^{-4}$. 


## BC: Exercise 2.2

**Consider the following experiment. Hold a penny on edge on a flat hard surface, and spin it with your fingers. Let $p$ denote the probability that it lands heads. To estimate this probability, we will use a histogram to model our prior beliefs about $p$. Divide the interval $[0,1]$ into the ten subintervals $[0, 0.1]$, $[0.1, 0.2]$, $...$, $[0.9, 1]$, and specify probabilities that p is in each interval. Next spin the penny 20 times and count the number of successes (heads) and failures (tails). Simulate from the posterior distribution by (1) computing the posterior density of $p$ on a grid of values on $(0, 1)$ and (2) taking a simulated sample with replacement from the grid. (The functions `histprior` and `sample` are helpful in this computation.) How have the interval probabilities changed on the basis of your data?**

At first, considering the ten subintervals $[0, 0.1]$, $[0.1, 0.2]$, $...$, $[0.9, 1]$, let's consider the center of these subintervals 
```{r}
library(LearnBayes)

midp <- seq(0.05, 0.95, by=0.1)
midp
```

Once defined these points we can express our prior beliefs about $p$ by using an histogram model that associate to each subinterval a probability by using the `histprior` function.
Since we don't know in advance if the coin is fair or not, we assume that the probability that it lands heads is equal for every subinterval. 

```{r}
prior <- c(.1,.1,.1,.1,.1,.1,.1,.1,.1,.1)
p <- seq(0,1,length=500)

prior <- histprior(midpts = midp, prob = prior,p)

plot(p, prior, type = "l", main = "Prior probability", xlab = "p")
```

After having stated our prior beliefs, we proceed by the experimental evaluation. We tossed a coin for 20 times and we reported in the following table the results:

 Throw        |   Result        | 
| ------------- |:-------------:| 
| 1      | Head | 
| 2      | Tail | 
| 3      | Tail | 
| 4      | Head | 
| 5      | Tail | 
| 6      | Head | 
| 7      | Head | 
| 8      | Head | 
| 9      | Tail | 
| 10      | Tail | 
| 11      | Head | 
| 12      | Tail | 
| 13      | Head | 
| 14      | Head | 
| 15      | Tail | 
| 16      | Head | 
| 17      | Tail | 
| 18      | Tail | 
| 19      | Head | 
| 20      | Head | 

**Total Heads ( $s$ )**: 11

**Total Tails ( $f$ )**: 9

We now update our knowledge of $p$ by applying the Bayes' theorem:

$$P(p|s) = \frac{P(s | p)P(p)}{\int dp P(s| p)P(p) }.$$

For what concerns the likelihood, we assume that the probability $P(s | p)$ to have $s$ successes in a 20-throw experiment follows a Binomial distribution:

$$P(s | p) = \binom{20}{s} ~ p^s (1-p)^{20-s}$$.

We evaluate this posterior in two different ways:

* evaluating the posterior $P(p|s)$ on a sequence of 500 elements that goes from 0 till 1.

```{r}
success <- 11
failure <- 9

like <- choose(20, success)*p^(success)*(1-p)^(failure)
post <- like*prior
post <- post/sum(post)

plot(p, post, main="Posterior probability", xlab="p")
```

* sampling $p$ with replacement with probability equal to the posterior.

```{r}
set.seed(123)
simulated <- sample(p, replace=TRUE, prob=post)
hist(simulated, xlab='p')
```

After the experiment, our notion about the probability $p$ is improved. The posterior distribution on both cases suggests that the probability of having head is around $0.5$ and this lead to think that the coin is fair.


## DAAG: Exercise 7.7

Apply polynomial regression to the seismic timing data in the data frame geophones. Specifically, check the fits of linear, quadratic, cubic, and quartic (degree = 4) polynomial estimates of the expected thickness as a function of distance. What do you observe about the fitted quartic curve? Do any of the fitted curves capture the curvature of the data in the region where `distance` is large?

We start by loading the data frame and explore it.
```{r, echo=TRUE}
library(DAAG)
df <- geophones
distance <- df$distance
thickness <-  df$thickness
plot(thickness ~ distance)
```

As we can see, there is not a clear linear relationship between the distance and the thickness. Let's try to fit the four different models, as requested by the excercise, and see what happens.

```{r, echo=TRUE}

linear <- lm(thickness ~ distance)
quadratic <- lm(thickness ~ distance + I(distance^2))
cubic <- lm(thickness ~ distance + I(distance^2) + I(distance^3))
quartic <- lm(thickness ~ distance + I(distance^2) + + I(distance^3) + I(distance^4))

plot(distance, thickness, xlab="Distance", ylab="Fitted values")
points(distance, linear$fitted.values, col="2", pch=19)
points(distance, quadratic$fitted.values, col="3", pch=19)
points(distance, cubic$fitted.values, col="4", pch=19, cex=.5)
points(distance, quartic$fitted.values, col="6", pch=19)
legend("topright", c("linear","quadratic","cubic","quartic"), pch=19, 
       col=c("2", "3", "4", "6"))
```

From the plot above, we can observe that the straight line is inappropriate to describe the relationship, and the quadratic and the cubic curves are quite similar to each other but they cannot capture the behaviour of the data when the distance is large. The only one that is following the data is the polynomial of degree 4. However, polynomials can be effective when a curve of degree 2 or 3 is used, instead, polynomial curves with degree greater than 3 can be problematic.

A first problem of this type of models, as we can observe in the result below, is the correlation between coefficients, that is really high.
```{r, echo=TRUE}
summary(quartic, corr=TRUE)
```

Another issue is that high-degree polynomials tend to move up and down between the data values in a snake-like manner and this would lead to overfitting. Splines, or piecewise polynomials, are usually preferable to polynomials of degree greater than 3.