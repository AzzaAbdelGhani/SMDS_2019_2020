---
title: "Homework 5"
author: "Group J - Abdalghani, Corti, Solomita"
date: "22/06/2020"
output:
  html_document:
    toc: yes
  beamer_presentation:
    highlight: tango
  include: null
  ioslides_presentation:
    highlight: tango
  pdf_document:
    highlight: tango
    keep_tex: yes
    toc: yes
  slide_level: 2
  slidy_presentation:
    fig.height: 3
    fig.width: 4
    highlight: tango
header-includes:
- \usepackage{color}
- \definecolor{Purple}{HTML}{911146}
- \definecolor{Orange}{HTML}{CF4A30}
- \setbeamercolor{alerted text}{fg=Orange}
- \setbeamercolor{frametitle}{bg=Purple}
institute: University of Udine & University of Trieste
graphics: yes
subtitle: 
fontsize: 10pt
---
```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.align = 'center', warning=FALSE, message=FALSE, fig.asp=0.625, dev='png', global.par = TRUE, dev.args=list(pointsize=10), fig.path = 'figs/')
library(MASS)
```
```{r setup, include=FALSE}
library(knitr)
local({
  hook_plot = knit_hooks$get('plot')
  knit_hooks$set(plot = function(x, options) {
    paste0('\n\n----\n\n', hook_plot(x, options))
  })
})
```

## CS: Exercise 1.3

**Suppose that $$ Y \sim \mathcal{N}(\begin{bmatrix} 1 \\2 \end{bmatrix} ,\begin{bmatrix} 2 & 1 \\ 1 & 2 \end{bmatrix}) $$ Find the conditional p.d.f. of $Y1$ given that $Y1 + Y2 = 3$.**


*solution :* 

$Y$ is a $2 \times1$ random vector with distribution $\mathcal{N_2}(\mu,\Sigma)$ where $\mu = \begin{bmatrix} 1 \\2 \end{bmatrix}$  and $\Sigma = \begin{bmatrix} 2 & 1 \\ 1 & 2 \end{bmatrix}$. 

Reffering to (1.7) at section (1.6.2) in the book, $A Y \sim \mathcal{N}(A\mu, A\Sigma A^T)$, we can get that the joint distribution of 
$$AY = \begin{bmatrix} Y_1 \\ Y_1 + Y_2 \end{bmatrix}$$ 
is $\mathcal{N_2}(\mu^*, \Sigma^*)$, where $A$ is matrix of finite real constants : 
$$A = \begin{bmatrix} 1 & 0 \\ 1 & 1 \end{bmatrix}$$
The $2 \times 1$ mean vector $\mu^*$ is: 
$$\mu^* = A\mu =  \begin{bmatrix} 1 & 0 \\ 1 & 1 \end{bmatrix} \begin{bmatrix} 1 \\2 \end{bmatrix} = \begin{bmatrix} 1 \\3 \end{bmatrix}$$
and the $2 \times 2$ covariance matrix is : 
$$ \Sigma^*=A\Sigma A^T = \begin{bmatrix} 1 & 0 \\ 1 & 1 \end{bmatrix} \begin{bmatrix} 2 & 1 \\ 1 & 2 \end{bmatrix} \begin{bmatrix} 1 & 1 \\ 0 & 1 \end{bmatrix} = \begin{bmatrix} 2 & 3 \\ 3 & 6 \end{bmatrix} $$

In section (1.6.3), we got the conditional p.d.f of 2 random vectors with multivariate normal joint distribution. Thus we can apply this to the distribution of $AY$, the conditional distribution of $Y_1 | Y_1 + Y_2 = 3$ is $\mathcal{N}(\mu_c, \sigma^2_c)$ , where the conditional mean $\mu_c$ is : 
$$\mu_c = \mu^*_1+ \Sigma^*_{12} \Sigma^{*-1}_{22} (c_2 - \mu^*_2) = 1 + 3.(6)^{-1} .(3-3) = 1$$

and the conditional variance $\sigma^2_c$ is : 

$$\sigma^2_c = \Sigma^*_{11} - \Sigma^*_{12}\Sigma^{*-1}_{22}\Sigma^*_{21} = 2 - 3. (6)^{-1}.3 = 0.5$$

So, the conditional p.d.f. of $Y1$ given $Y1 + Y2 = 3$ is $\mathcal{N}(1,\frac{1}{2})$

## DAAG: Exercise 4.5

**The following code draws, in a 2×2 layout, 10 boxplots of random samples of 1000 from anormal distribution, 10 boxplots of random samples of 1000 from at-distribution with 7 d.f.,10 boxplots of random samples of 200 from a normal distribution, and 10 boxplots of randomsamples of 200 from at-distribution with 7 d.f. :**

```{r echo=TRUE,fig.show='hide',message=FALSE, warning=FALSE}
oldpar <- par(mfrow=c(2,2))
tenfold1000 <- rep(1:10, rep(1000,10))
boxplot(split(rnorm(1000*10), tenfold1000), ylab="normal - 1000")
boxplot(split(rt(1000*10, 7), tenfold1000),ylab=expression(t[7]*" - 1000"))
tenfold100 <- rep(1:10, rep(100, 10))
boxplot(split(rnorm(100*10), tenfold100), ylab="normal - 100")
boxplot(split(rt(100*10, 7), tenfold100),ylab=expression(t[7]*" - 100"))
par(oldpar)
```

**Refer back to the discussion of heavy-tailed distributions in Subsection 3.2.2, and commenton the different numbers and configurations of points that are flagged as possible outliers**.

```{r}
oldpar <- par(mfrow=c(2,2))
tenfold1000 <- rep(1:10, rep(1000,10))
boxplot(split(rnorm(1000*10), tenfold1000), ylab="normal - 1000")
boxplot(split(rt(1000*10, 7), tenfold1000),ylab=expression(t[7]*" - 1000"))
tenfold100 <- rep(1:10, rep(100, 10))
boxplot(split(rnorm(100*10), tenfold100), ylab="normal - 100")
boxplot(split(rt(100*10, 7), tenfold100),ylab=expression(t[7]*" - 100"))
par(oldpar)
```


Comparing normal and t-distribution we observe that there's a tendency to have a higher number of outliers in the samples simulated from a t distribution rather than the normal.
This is due to the analytical form of the probability density function; in fact the pdf of a Normal reports an exponential decay and this property takes as consequence a probability that decays very rapidly as far as we go out of the mean $\mu$. This lead to have a very low probability to sample values that are outside to the interval of the mean.
On the other hand, the t-distribution with 7 df doesn't have this property; these kind of distributions are known as **heavy-tailed distributions**. This lead to have an higher probability to sample points that are outside  the most frequent values.


In order to see how big is this difference consider this function that prints us the ratio $\frac{1-p_t(x \in [-a,+a])}{1-p_{norm}(x \in [-a,+a])}$ between the probabilities of a $t_7$ and a $\mathcal{N}(0,1)$ distribution to be out of a certain interval $[-a,+a]$ for $a =1,2,2.5,2.8,3$
```{r}
ratio_probability <- function(level){
  prob_norm <- 1-(pnorm(level,mean=0,sd=1)-pnorm(-level,mean=0,sd=1))
  prob_t <- 1-(pt(level,df=7)-pt(-level,df=7))
  return(prob_t/prob_norm)
}
```
```{r}
a<-c(1,2,2.5,2.8,3)

ratio_probability(a)
```

As we can observe from these probability ratios that a value out the region of $x\sim 0$ is more likely for the t-distribution and this difference is larger as far as we move out. 
In fact, the probability to sample a value $x \notin [-3,3]$ from a t distribution is more than $7$ times larger than in the normal case.



## CS: Exercise 4.4

**Suppose that you have $n$ independent measurements of times between major aircraft disasters, $t_i$, and believe that the probability density function for the $t_i$’s is of the form: $f(t) = k e^{-\lambda t^2}$ $t \geq 0$ where $\lambda$ and $k$ are the same for all $i$.**

**(a) By considering the normal p.d.f., show that $k = \sqrt{4 \lambda / \pi}$. **

The normal p.d.f is : 

$$ f(t) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp\{-\frac{(x - \mu)^2}{2\sigma^2}\}) $$
By considering the mean of this normal p.d.f is 0 ($\mu = 0$) and $x$ takes only positive values, since $t = x - \mu = x \geq 0$ : 
$$ f(t) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp\{-\frac{t^2}{2\sigma^2}\}) $$

From this equation, we can consider $k = \frac{1}{\sqrt{2\pi\sigma^2}}$ and $\lambda = \frac{1}{2\sigma^2}$, then the f(t) will be as the p.d.f of $t_i$: 

$$f(t) = k e^{-\lambda t^2}$$ . 

Now, to prove that $k = \sqrt{4 \lambda / \pi}$, we can write $k$ with rescpect to $\lambda$ : 

$$k = \frac{1}{\sqrt{2\pi\sigma^2}} = \sqrt{\frac{1}{2\pi\sigma^2}} = \sqrt{\frac{1}{2\sigma^2}\frac{1}{\pi}} = \sqrt{\lambda/\pi}$$

In this case, we can see that $k = \sqrt{ \lambda / \pi}$ , and to make it $\sqrt{4 \lambda / \pi}$ we have to multiply the normal p.d.f by 2, such that : 

$$ f(t) = \frac{2}{\sqrt{2\pi\sigma^2}}\exp\{-\frac{(x - \mu)^2}{2\sigma^2}\}) $$

where again $t = x - \mu$ , $k = \frac{2}{\sqrt{2\pi\sigma^2}}$ and $\lambda = \frac{1}{2\sigma^2}$. And in this case : 

$$k = \frac{2}{\sqrt{2\pi\sigma^2}} = \sqrt{\frac{4}{2\pi\sigma^2}} = \sqrt{4 \frac{1}{2\sigma^2}\frac{1}{\pi}} = \sqrt{4\lambda/\pi}$$



**(b) Obtain a maximum likelihood estimator for $\lambda$. ** 

To obtain the MLE for $\lambda$ : 

1. Compute the likelihood function for the function f(t):

$$ L(t) = \prod_{i = 0}^nke^{-\lambda t_i^2} = k^n \prod_{i = 0}^n e^{-\lambda t_i^2}$$

2. Take the logarithm of the Likelihood function to get the log likelihood : 


$$ \ell(t) = \log (k^n \prod_{i = 0}^n e^{-\lambda t_i^2}) =  \log (k^n) + \log (\prod_{i = 0}^n e^{-\lambda t_i^2}) =n \log(k) - \lambda \sum_{i = 0}^n t_i^2 = n \log(\sqrt{4\lambda/\pi}) - \lambda \sum_{i = 0}^n t_i^2$$

3. Set the derivative of the log likelihood to 0:

$$\frac{d\ell(t)}{d\lambda} = \frac{n}{2\lambda} - \sum_{i = 0}^n t_i^2 = 0$$
$$\frac{n}{2\lambda} = \sum_{i = 0}^n t_i^2$$

$$\lambda = \frac{n}{2 \sum_{i = 0}^n t_i^2}$$
which is the Maximum Likelihood estimator for $\lambda$. 


**(c) Given observations of $T_i$ (in days) of: 243, 14, 121, 63, 45, 407 and 34, use a generalised likelihood ratio test to test $H_0 : \lambda  = 10^{-4}$ against the alternative of no restriction on $\lambda$ at the 5% significance level. Note that if $V ∼ χ_{1}^2$ then $Pr[V ≤ 3.841] = 0.95$. **




The log_likelihood is : $\ell(t) = n \log(\sqrt{4\lambda/\pi}) - \lambda \sum_{i = 0}^n t_i^2$, we need to replace $\lambda$ parameter once with the value of null hypothesis $\lambda  = 10^{-4}$ and the other with the value of MLE that we have found before as an alternative of no restriction on $\lambda$.
 
Then, the likelihood ratio test $LRT$ would become : 
$$ W(H_0) = 2 \{\ell(\hat\lambda) - \ell(\hat\lambda_{H_0})\} $$
 

```{r echo=TRUE,  message=FALSE, warning=FALSE}

log_likelihood <- function(data, param){
   length(data) * log(sqrt(4 * param / pi)) - param * sum(data^2)
 }

obs <- c(243, 14, 121, 63, 45, 407, 34)

lambda_mle <- length(obs) / (2 * sum(obs^2))
print(paste("MLE for lambda = ", lambda_mle))

l_H0 <- log_likelihood(obs, 10^-4)
l_mle <- log_likelihood(obs, lambda_mle)

lrt <- 2 * (l_mle - l_H0)

p_lrt <- pchisq(lrt, df=1, lower.tail = FALSE)
print(paste("p-value of Likelihood ratio test :", p_lrt))
```

Based on the p-value, as it is too low, we reject the null hypothesis. AS, also we can see that the MLE for $\lambda$ is different from $10^{-4}$. 


## BC: Exercise 2.2

**Consider the following experiment. Hold a penny on edge on a flat hard surface, and spin it with your fingers. Let $p$ denote the probability that it lands heads. To estimate this probability, we will use a histogram to model our prior beliefs about $p$. Divide the interval $[0,1]$ into the ten subintervals $[0, 0.1]$, $[0.1, 0.2]$, $...$, $[0.9, 1]$, and specify probabilities that p is in each interval. Next spin the penny 20 times and count the number of successes (heads) and failures (tails). Simulate from the posterior distribution by (1) computing the posterior density of $p$ on a grid of values on $(0, 1)$ and (2) taking a simulated sample with replacement from the grid. (The functions `histprior` and `sample` are helpful in this computation.) How have the interval probabilities changed on the basis of your data?**

At first, considering the ten subintervals $[0, 0.1]$, $[0.1, 0.2]$, $...$, $[0.9, 1]$, let's consider the center of these subintervals 
```{r}
library(LearnBayes)

midp<-seq(0.05,0.95, by=0.1)
midp
```
Once defined these points we can express our prior beliefs about $p$ by using an histogram model that associate to each subinterval a probability by using the `histprior` function.
Since we don't know in advance if the coin is fair or not, we assume that the probability that it lands heads is equal for every subinterval. 
```{r}
prior <- c(.1,.1,.1,.1,.1,
           .1,.1,.1,.1,.1)
p=seq(0,1,length=500)

prior<-histprior(midpts = midp, prob = prior,p)

plot(p,prior, type = "l", main = "Prior probability", xlab = "p")
```

After having stated our prior beliefs, we proceed by the experimental evaluation. We tossed a coin for 20 times and we reported in the following table the results:

 Throw        |   Result        | 
| ------------- |:-------------:| 
| 1      | Head | 
| 2      | Tail | 
| 3      | Tail | 
| 4      | Head | 
| 5      | Tail | 
| 6      | Head | 
| 7      | Head | 
| 8      | Head | 
| 9      | Tail | 
| 10      | Tail | 
| 11      | Head | 
| 12      | Tail | 
| 13      | Head | 
| 14      | Head | 
| 15      | Tail | 
| 16      | Head | 
| 17      | Tail | 
| 18      | Tail | 
| 19      | Head | 
| 20      | Head | 

**Total Heads ($s$)**: 11
**Total Tails ($f$)**: 9

We now update our knowledge of $p$ by applying the Bayes' theorem:

$$P(p|s) = \frac{P(s | p)P(p)}{\int dp P(s| p)P(p) }.$$

For what concerns the likelihood, we assume that the probability $P(s | p)$ to have $s$ successes in a 20-throw experiment follows a Binomial distribution:

$$P(s | p) = \binom{20}{s} ~ p^s (1-p)^{20-s}$$.

We evaluate this posterior in two different ways:

* evaluating the posterior $P(p|s)$ on a sequence of 500 elements that goes from 0 till 1.

```{r}
success<-11
failure<-9

like<-choose(20, success)*p^(success)*(1-p)^(failure)
post<-like*prior
post<-post/sum(post)

plot(p, post, main="Posterior probability", xlab="p")
```

* sampling $p$ with replacement with probability equal to the posterior.

```{r}
Simulated<-sample(p,replace=TRUE, prob=post)
hist(Simulated, xlab='p')
```

After the experiment, our notion about the probability $p$ is improved. The posterior distribution on both cases suggests that the probability of having head is around $0.5$ and this lead to think that the coin is fair.

## DAAG: Exercise 7.7
